{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "split = \"test\"\n",
    "dest_path = \"/liuzicheng/ljh/hyena-dna/data/splicing_prediction\"\n",
    "all_seqs = np.load(os.path.join(dest_path,  split + \"_seq.npy\"), allow_pickle=True)\n",
    "all_labels = np.load(os.path.join(dest_path,  split + \"_target.npy\"), allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyena_path='/liuzicheng/ljh/hyena-dna/outputs/2024-04-03/15-12-52-724312/checkpoints/val/pr_auc_mean.ckpt'\n",
    "mamba_path='/liuzicheng/ljh/hyena-dna/outputs/2024-04-20/02-29-01-651292/checkpoints/val/pr_auc_mean.ckpt'\n",
    "NT_path='/liuzicheng/ljh/hyena-dna/outputs/2024-04-08/01-41-26-952782/checkpoints/val/pr_auc_mean.ckpt'\n",
    "genalm_path='/liuzicheng/ljh/hyena-dna/outputs/2024-04-06/11-21-47-532494/checkpoints/val/pr_auc_mean.ckpt'\n",
    "bert2_path='/liuzicheng/ljh/hyena-dna/outputs/2024-04-02/18-41-34-140615/checkpoints/val/pr_auc_mean.ckpt'\n",
    "batch_size=1\n",
    "max_length=3000\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "def pr_auc(logits,y):\n",
    "    # compute metrics based on stored labels, predictions, ...\n",
    "    metrics = {}\n",
    "    y, p = y, torch.sigmoid(logits)\n",
    "    #convert three dimension into 2 dimension\n",
    "    y=y.reshape(-1,y.shape[-1]).cpu().numpy()\n",
    "    p=p.reshape(-1,p.shape[-1]).cpu().detach().numpy()\n",
    "    # compute pr-auc for each class independetly\n",
    "    for label in [0, 1, 2]:\n",
    "        y_label = y[:, label]\n",
    "        p_label = p[:, label]\n",
    "        if not np.isnan(p_label).any():\n",
    "            try:\n",
    "                pr_auc = roc_auc_score(y_label, p_label)\n",
    "            except ValueError:\n",
    "                #calculate accurate rate for this label\n",
    "                #convert p_label to 0-1\n",
    "                p_label = (p_label > 0.5).astype(int)\n",
    "                accurate_rate = (y_label == p_label).sum() / len(y_label)\n",
    "                #if accurate rate is 1, set pr_auc to 1\n",
    "                pr_auc = accurate_rate\n",
    "        else:\n",
    "            pr_auc = np.nan\n",
    "        # to be compatible with sklearn 1.1+\n",
    "        metrics[f'pr_auc_{label}'] = pr_auc if not np.isnan(pr_auc) else 0.0\n",
    "    metrics['pr_auc_mean'] = (metrics['pr_auc_1'] + metrics['pr_auc_2']+metrics['pr_auc_0']) / 3\n",
    "    return metrics\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    if y.numel() > logits.shape[0]:\n",
    "        # Mixup leads to this case: use argmax class\n",
    "        y = y.argmax(dim=-1)\n",
    "    y = y.view(-1)\n",
    "    return torch.eq(preds, y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "{'pr_auc_0': 0.5154864462490805, 'pr_auc_1': 0.5590537978461373, 'pr_auc_2': 0.6995160261908133, 'pr_auc_mean': 0.5913520900953437}\n",
      "tensor(0.9996)\n",
      "2000\n",
      "{'pr_auc_0': 0.5141785628747122, 'pr_auc_1': 0.5923108638222884, 'pr_auc_2': 0.7170157376860257, 'pr_auc_mean': 0.6078350547943421}\n",
      "tensor(0.9996)\n",
      "3000\n",
      "{'pr_auc_0': 0.5525250050826079, 'pr_auc_1': 0.5910793756973244, 'pr_auc_2': 0.7207239376237361, 'pr_auc_mean': 0.6214427728012227}\n",
      "tensor(0.9996)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m seq_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_seqs)):\n\u001b[0;32m---> 30\u001b[0m     sequence_encoded\u001b[38;5;241m=\u001b[39mhyena_tokenizer(all_seqs[i],\n\u001b[1;32m     31\u001b[0m                         add_special_tokens\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# this is what controls adding eos\u001b[39;00m\n\u001b[1;32m     32\u001b[0m                         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m                         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m     34\u001b[0m                         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m                     )\n\u001b[1;32m     36\u001b[0m     seq_ids\u001b[38;5;241m=\u001b[39msequence_encoded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     37\u001b[0m     seq_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(seq_ids)\n",
      "File \u001b[0;32m/liuzicheng/anaconda3/envs/evo-design/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2858\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2858\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2860\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/liuzicheng/anaconda3/envs/evo-design/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2964\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2945\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2946\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2961\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2962\u001b[0m     )\n\u001b[1;32m   2963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2965\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2966\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2967\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2968\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2969\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2970\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2971\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2972\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2973\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2974\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2975\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2976\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2977\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2978\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2979\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2980\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2981\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2982\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2983\u001b[0m     )\n",
      "File \u001b[0;32m/liuzicheng/anaconda3/envs/evo-design/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3037\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3028\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3029\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3030\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3034\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3035\u001b[0m )\n\u001b[0;32m-> 3037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[1;32m   3038\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3039\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   3040\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   3041\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   3042\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   3043\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   3044\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   3045\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   3046\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   3047\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   3048\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   3049\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   3050\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   3051\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   3052\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   3053\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   3054\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   3055\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3056\u001b[0m )\n",
      "File \u001b[0;32m/liuzicheng/anaconda3/envs/evo-design/lib/python3.11/site-packages/transformers/tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[0;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    723\u001b[0m     first_ids,\n\u001b[1;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    739\u001b[0m )\n",
      "File \u001b[0;32m/liuzicheng/anaconda3/envs/evo-design/lib/python3.11/site-packages/transformers/tokenization_utils.py:687\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    686\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_split_into_words:\n",
      "File \u001b[0;32m/liuzicheng/anaconda3/envs/evo-design/lib/python3.11/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    647\u001b[0m ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m--> 649\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_token_to_id_with_added_voc(token))\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m/liuzicheng/anaconda3/envs/evo-design/lib/python3.11/site-packages/transformers/tokenization_utils.py:652\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    649\u001b[0m         ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_token_to_id_with_added_voc(token))\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ids\n\u001b[0;32m--> 652\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_token_to_id_with_added_voc\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    state_dict='/liuzicheng/ljh/hyena-dna/weight/hyenadna/hyenadna-large-1m-seqlen'\n",
    "    hyena_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    hyena_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(hyena_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    hyena_decoder = nn.Linear(256,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    hyena_model.load_state_dict(checkpoint,strict=False)\n",
    "    hyena_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    hyena_model.eval()\n",
    "    hyena_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "        sequence_encoded=hyena_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=hyena_model(input_ids=seqs).last_hidden_state\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=hyena_decoder(hidden_states)\n",
    "        out1_hyena=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_hyena)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "                accuracy_value=accuracy(seq_list_tensor,target_list_tensor)\n",
    "                print(accuracy_value)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/modules/transformers_modules/dnabert2/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /liuzicheng/ljh/hyena-dna/weight/dnabert2 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/root/.cache/huggingface/modules/transformers_modules/dnabert2/bert_layers.py:433: UserWarning: Increasing alibi size from 512 to 3000\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "{'pr_auc_0': 0.4413639488404158, 'pr_auc_1': 0.6495801693186615, 'pr_auc_2': 0.7035993527238612, 'pr_auc_mean': 0.5981811569609795}\n",
      "2000\n",
      "{'pr_auc_0': 0.4833810037010731, 'pr_auc_1': 0.6579949162945948, 'pr_auc_2': 0.7102444025825281, 'pr_auc_mean': 0.617206774192732}\n",
      "3000\n",
      "{'pr_auc_0': 0.5768825078913379, 'pr_auc_1': 0.6441416139443331, 'pr_auc_2': 0.7067245205190908, 'pr_auc_mean': 0.6425828807849205}\n",
      "4000\n",
      "{'pr_auc_0': 0.6094680668758002, 'pr_auc_1': 0.6325235272635453, 'pr_auc_2': 0.7039271875574811, 'pr_auc_mean': 0.6486395938989422}\n",
      "5000\n",
      "{'pr_auc_0': 0.6276391536145002, 'pr_auc_1': 0.6317029275923831, 'pr_auc_2': 0.7048822524375558, 'pr_auc_mean': 0.6547414445481463}\n",
      "6000\n",
      "{'pr_auc_0': 0.6320016902614787, 'pr_auc_1': 0.6241559409749575, 'pr_auc_2': 0.6982471967887951, 'pr_auc_mean': 0.6514682760084104}\n",
      "7000\n",
      "{'pr_auc_0': 0.6455787283720203, 'pr_auc_1': 0.6287807207037904, 'pr_auc_2': 0.7012021169564384, 'pr_auc_mean': 0.6585205220107498}\n",
      "8000\n",
      "{'pr_auc_0': 0.6472556370414385, 'pr_auc_1': 0.6290027698675833, 'pr_auc_2': 0.703146369672784, 'pr_auc_mean': 0.6598015921939352}\n",
      "9000\n",
      "{'pr_auc_0': 0.6453219208812204, 'pr_auc_1': 0.6288617347625843, 'pr_auc_2': 0.7007478522560913, 'pr_auc_mean': 0.6583105026332986}\n",
      "10000\n",
      "{'pr_auc_0': 0.6470114523640572, 'pr_auc_1': 0.6358241896759768, 'pr_auc_2': 0.7035487965867486, 'pr_auc_mean': 0.6621281462089276}\n",
      "11000\n",
      "{'pr_auc_0': 0.6390487075744324, 'pr_auc_1': 0.6360777592555877, 'pr_auc_2': 0.7045179479767065, 'pr_auc_mean': 0.6598814716022422}\n",
      "12000\n",
      "{'pr_auc_0': 0.626828593713372, 'pr_auc_1': 0.6380907263073617, 'pr_auc_2': 0.7059928624351631, 'pr_auc_mean': 0.6569707274852989}\n",
      "13000\n",
      "{'pr_auc_0': 0.6336316624652701, 'pr_auc_1': 0.6381459072859856, 'pr_auc_2': 0.7070680443183892, 'pr_auc_mean': 0.6596152046898817}\n",
      "14000\n",
      "{'pr_auc_0': 0.6384097449602768, 'pr_auc_1': 0.6372235322340396, 'pr_auc_2': 0.7073622405396568, 'pr_auc_mean': 0.6609985059113245}\n",
      "15000\n",
      "{'pr_auc_0': 0.6440681559117699, 'pr_auc_1': 0.6339020425904798, 'pr_auc_2': 0.706046603988664, 'pr_auc_mean': 0.6613389341636379}\n",
      "16000\n",
      "{'pr_auc_0': 0.6460772733413054, 'pr_auc_1': 0.6352172297748268, 'pr_auc_2': 0.7079444804071149, 'pr_auc_mean': 0.6630796611744156}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='/liuzicheng/ljh/hyena-dna/weight/dnabert2'\n",
    "    bert2_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    bert2_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(bert2_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    bert2_decoder = nn.Linear(768,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    bert2_model.load_state_dict(checkpoint,strict=False)\n",
    "    bert2_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    bert2_model.eval()\n",
    "    bert2_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=bert2_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=bert2_model(input_ids=seqs,export_hidden_states=True)[0]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=bert2_decoder(hidden_states)\n",
    "        out1_bert2=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_bert2)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "{'pr_auc_0': 0.7880212471259458, 'pr_auc_1': 0.6770300582523869, 'pr_auc_2': 0.7187810802183864, 'pr_auc_mean': 0.7279441285322396}\n",
      "2000\n",
      "{'pr_auc_0': 0.8167638615468158, 'pr_auc_1': 0.6561324144945506, 'pr_auc_2': 0.7148380653496311, 'pr_auc_mean': 0.7292447804636658}\n",
      "3000\n",
      "{'pr_auc_0': 0.8478734619509036, 'pr_auc_1': 0.6513071565600782, 'pr_auc_2': 0.7241171135823166, 'pr_auc_mean': 0.7410992440310995}\n",
      "4000\n",
      "{'pr_auc_0': 0.851880441791636, 'pr_auc_1': 0.6420799196727746, 'pr_auc_2': 0.7283626722476464, 'pr_auc_mean': 0.7407743445706857}\n",
      "5000\n",
      "{'pr_auc_0': 0.8597831673439507, 'pr_auc_1': 0.6354504501119524, 'pr_auc_2': 0.7304985274249985, 'pr_auc_mean': 0.7419107149603006}\n",
      "6000\n",
      "{'pr_auc_0': 0.8677362138412498, 'pr_auc_1': 0.6283971543377364, 'pr_auc_2': 0.7241814758055377, 'pr_auc_mean': 0.7401049479948413}\n",
      "7000\n",
      "{'pr_auc_0': 0.8611445707305545, 'pr_auc_1': 0.6311706598652306, 'pr_auc_2': 0.7273982069457894, 'pr_auc_mean': 0.7399044791805247}\n",
      "8000\n",
      "{'pr_auc_0': 0.8626132615327134, 'pr_auc_1': 0.6307871423897916, 'pr_auc_2': 0.7270191051685777, 'pr_auc_mean': 0.7401398363636943}\n",
      "9000\n",
      "{'pr_auc_0': 0.8637529334097808, 'pr_auc_1': 0.6275317471304522, 'pr_auc_2': 0.7256472634547031, 'pr_auc_mean': 0.7389773146649787}\n",
      "10000\n",
      "{'pr_auc_0': 0.8660732716980265, 'pr_auc_1': 0.632267222086135, 'pr_auc_2': 0.726126224183549, 'pr_auc_mean': 0.7414889059892368}\n",
      "11000\n",
      "{'pr_auc_0': 0.8634528897797865, 'pr_auc_1': 0.6318687763955713, 'pr_auc_2': 0.7274922765831349, 'pr_auc_mean': 0.7409379809194975}\n",
      "12000\n",
      "{'pr_auc_0': 0.8624209168105471, 'pr_auc_1': 0.6325031604480222, 'pr_auc_2': 0.7280860023792299, 'pr_auc_mean': 0.7410033598792665}\n",
      "13000\n",
      "{'pr_auc_0': 0.8617312584465523, 'pr_auc_1': 0.6318345905976037, 'pr_auc_2': 0.7295563679336788, 'pr_auc_mean': 0.7410407389926116}\n",
      "14000\n",
      "{'pr_auc_0': 0.8629787419300166, 'pr_auc_1': 0.629811678170904, 'pr_auc_2': 0.7319049070207474, 'pr_auc_mean': 0.7415651090405561}\n",
      "15000\n",
      "{'pr_auc_0': 0.8648920318875633, 'pr_auc_1': 0.6275538667357794, 'pr_auc_2': 0.7294159770745242, 'pr_auc_mean': 0.7406206252326223}\n",
      "16000\n",
      "{'pr_auc_0': 0.8667097413780834, 'pr_auc_1': 0.6298151777649452, 'pr_auc_2': 0.7303904047683528, 'pr_auc_mean': 0.7423051079704606}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='/liuzicheng/ljh/hyena-dna/weight/genalm/gena-lm-bigbird-base-t2t'\n",
    "    genalm_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    genalm_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(genalm_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    genalm_decoder = nn.Linear(768,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    genalm_model.load_state_dict(checkpoint,strict=False)\n",
    "    genalm_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    genalm_model.eval()\n",
    "    genalm_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=genalm_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=genalm_model(input_ids=seqs, output_hidden_states=True,).hidden_states[-1]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=genalm_decoder(hidden_states)\n",
    "        out1_genalm=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_genalm)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/liuzicheng/anaconda3/envs/evo-design/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "{'pr_auc_0': 0.5528131862270298, 'pr_auc_1': 0.5463983841379774, 'pr_auc_2': 0.7100390064366782, 'pr_auc_mean': 0.6030835256005618}\n",
      "2000\n",
      "{'pr_auc_0': 0.5644413401418739, 'pr_auc_1': 0.5677631092582998, 'pr_auc_2': 0.7177888964563967, 'pr_auc_mean': 0.6166644486188567}\n",
      "3000\n",
      "{'pr_auc_0': 0.579306468454271, 'pr_auc_1': 0.5640886874766704, 'pr_auc_2': 0.7190675758194758, 'pr_auc_mean': 0.6208209105834724}\n",
      "4000\n",
      "{'pr_auc_0': 0.6037103479946816, 'pr_auc_1': 0.5592286949901967, 'pr_auc_2': 0.7166983058490674, 'pr_auc_mean': 0.6265457829446485}\n",
      "5000\n",
      "{'pr_auc_0': 0.6150882743431582, 'pr_auc_1': 0.5549603588757929, 'pr_auc_2': 0.7193207428935381, 'pr_auc_mean': 0.6297897920374963}\n",
      "6000\n",
      "{'pr_auc_0': 0.6189362088925453, 'pr_auc_1': 0.5541015259108296, 'pr_auc_2': 0.7131134184647558, 'pr_auc_mean': 0.6287170510893768}\n",
      "7000\n",
      "{'pr_auc_0': 0.6063457937701472, 'pr_auc_1': 0.5553964152469485, 'pr_auc_2': 0.7159778537577799, 'pr_auc_mean': 0.6259066875916252}\n",
      "8000\n",
      "{'pr_auc_0': 0.5966400669665272, 'pr_auc_1': 0.5562500540684906, 'pr_auc_2': 0.7176010809637943, 'pr_auc_mean': 0.6234970673329374}\n",
      "9000\n",
      "{'pr_auc_0': 0.607683189912109, 'pr_auc_1': 0.5551425574418142, 'pr_auc_2': 0.7154522026811555, 'pr_auc_mean': 0.6260926500116929}\n",
      "10000\n",
      "{'pr_auc_0': 0.6041951931008084, 'pr_auc_1': 0.560952194472724, 'pr_auc_2': 0.7173576754981495, 'pr_auc_mean': 0.6275016876905607}\n",
      "11000\n",
      "{'pr_auc_0': 0.5997031592340297, 'pr_auc_1': 0.5587070647329252, 'pr_auc_2': 0.720139728197444, 'pr_auc_mean': 0.6261833173881329}\n",
      "12000\n",
      "{'pr_auc_0': 0.5962056718583916, 'pr_auc_1': 0.5580489234394572, 'pr_auc_2': 0.7218087027877446, 'pr_auc_mean': 0.6253544326951977}\n",
      "13000\n",
      "{'pr_auc_0': 0.5841996033597957, 'pr_auc_1': 0.559713211928514, 'pr_auc_2': 0.7224069678702488, 'pr_auc_mean': 0.6221065943861862}\n",
      "14000\n",
      "{'pr_auc_0': 0.5856330637293167, 'pr_auc_1': 0.5568974062057541, 'pr_auc_2': 0.7236996650630781, 'pr_auc_mean': 0.6220767116660496}\n",
      "15000\n",
      "{'pr_auc_0': 0.5934692525066468, 'pr_auc_1': 0.5590791516106526, 'pr_auc_2': 0.7219845828145837, 'pr_auc_mean': 0.6248443289772944}\n",
      "16000\n",
      "{'pr_auc_0': 0.5968385075117434, 'pr_auc_1': 0.5573107557194397, 'pr_auc_2': 0.7222753573388417, 'pr_auc_mean': 0.6254748735233416}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='/liuzicheng/ljh/hyena-dna/weight/nt/nucleotide-transformer-v2-500m-multi-species'\n",
    "    nt_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    nt_model=AutoModelForMaskedLM.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(NT_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    nt_decoder = nn.Linear(1024,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    nt_model.load_state_dict(checkpoint,strict=False)\n",
    "    nt_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    nt_model.eval()\n",
    "    nt_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=nt_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=nt_model(input_ids=seqs,output_hidden_states=True)['hidden_states'][-1]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=nt_decoder(hidden_states)\n",
    "        out1_nt=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_nt)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "{'pr_auc_0': 0.5388617909092803, 'pr_auc_1': 0.6441216147249552, 'pr_auc_2': 0.7270903694081531, 'pr_auc_mean': 0.6366912583474629}\n",
      "2000\n",
      "{'pr_auc_0': 0.5433083804365973, 'pr_auc_1': 0.6490626569738048, 'pr_auc_2': 0.7270350315210441, 'pr_auc_mean': 0.6398020229771487}\n",
      "3000\n",
      "{'pr_auc_0': 0.6076680475028369, 'pr_auc_1': 0.6532792935822576, 'pr_auc_2': 0.7362385209274198, 'pr_auc_mean': 0.665728620670838}\n",
      "4000\n",
      "{'pr_auc_0': 0.6358498053252069, 'pr_auc_1': 0.6416271479971765, 'pr_auc_2': 0.7351133119952585, 'pr_auc_mean': 0.6708634217725473}\n",
      "5000\n",
      "{'pr_auc_0': 0.6456392775320324, 'pr_auc_1': 0.6457690367912082, 'pr_auc_2': 0.74072134952049, 'pr_auc_mean': 0.6773765546145768}\n",
      "6000\n",
      "{'pr_auc_0': 0.6539994429578858, 'pr_auc_1': 0.6367329557369094, 'pr_auc_2': 0.7296913051219482, 'pr_auc_mean': 0.6734745679389144}\n",
      "7000\n",
      "{'pr_auc_0': 0.6613370710411016, 'pr_auc_1': 0.637248168292053, 'pr_auc_2': 0.7323301995997362, 'pr_auc_mean': 0.6769718129776302}\n",
      "8000\n",
      "{'pr_auc_0': 0.6480404693737357, 'pr_auc_1': 0.6405536509341219, 'pr_auc_2': 0.7353279553903195, 'pr_auc_mean': 0.6746406918993925}\n",
      "9000\n",
      "{'pr_auc_0': 0.6491514859673156, 'pr_auc_1': 0.6394663371346068, 'pr_auc_2': 0.7335216546331801, 'pr_auc_mean': 0.6740464925783675}\n",
      "10000\n",
      "{'pr_auc_0': 0.6419484634837973, 'pr_auc_1': 0.6456170870374889, 'pr_auc_2': 0.7351378507464348, 'pr_auc_mean': 0.6742344670892404}\n",
      "11000\n",
      "{'pr_auc_0': 0.6292723533935656, 'pr_auc_1': 0.6444731958546567, 'pr_auc_2': 0.7365319431616543, 'pr_auc_mean': 0.6700924974699589}\n",
      "12000\n",
      "{'pr_auc_0': 0.6219557656105064, 'pr_auc_1': 0.6445911043461708, 'pr_auc_2': 0.7376885302544558, 'pr_auc_mean': 0.6680784667370444}\n",
      "13000\n",
      "{'pr_auc_0': 0.626777598347018, 'pr_auc_1': 0.6451180759421373, 'pr_auc_2': 0.7390237079905422, 'pr_auc_mean': 0.6703064607598992}\n",
      "14000\n",
      "{'pr_auc_0': 0.6293666681751423, 'pr_auc_1': 0.6452830469886544, 'pr_auc_2': 0.7415181716893088, 'pr_auc_mean': 0.6720559622843685}\n",
      "15000\n",
      "{'pr_auc_0': 0.632472060685323, 'pr_auc_1': 0.6422158987991672, 'pr_auc_2': 0.739640491427456, 'pr_auc_mean': 0.6714428169706488}\n",
      "16000\n",
      "{'pr_auc_0': 0.6341306199611897, 'pr_auc_1': 0.6428918624938226, 'pr_auc_2': 0.7409143676595762, 'pr_auc_mean': 0.6726456167048629}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='/liuzicheng/ljh/hyena-dna/weight/mamba/caduceus-ph_seqlen-131k_d_model-256_n_layer-16'\n",
    "    mamba_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    mamba_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(mamba_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    mamba_decoder = nn.Linear(256,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    mamba_model.load_state_dict(checkpoint,strict=False)\n",
    "    mamba_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    mamba_model.eval()\n",
    "    mamba_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=mamba_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=mamba_model(seqs,output_hidden_states=True).last_hidden_state\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=mamba_decoder(hidden_states)\n",
    "        out1_mamba=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_mamba)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "{'pr_auc_0': 0.5012496497312322, 'pr_auc_1': 0.557875898625051, 'pr_auc_2': 0.6543104627247216, 'pr_auc_mean': 0.5711453370270017}\n",
      "2000\n",
      "{'pr_auc_0': 0.513053010096482, 'pr_auc_1': 0.5442053543889976, 'pr_auc_2': 0.6700020347770071, 'pr_auc_mean': 0.5757534664208289}\n",
      "3000\n",
      "{'pr_auc_0': 0.5559023029434687, 'pr_auc_1': 0.5489443397389752, 'pr_auc_2': 0.6737774871791081, 'pr_auc_mean': 0.5928747099538506}\n",
      "4000\n",
      "{'pr_auc_0': 0.5839581145781151, 'pr_auc_1': 0.5398353736083129, 'pr_auc_2': 0.6743729276059067, 'pr_auc_mean': 0.5993888052641115}\n",
      "5000\n",
      "{'pr_auc_0': 0.5937846155051626, 'pr_auc_1': 0.5435915583256301, 'pr_auc_2': 0.6789547593555947, 'pr_auc_mean': 0.6054436443954625}\n",
      "6000\n",
      "{'pr_auc_0': 0.583674049685009, 'pr_auc_1': 0.5411016133705435, 'pr_auc_2': 0.6767005084553682, 'pr_auc_mean': 0.600492057170307}\n",
      "7000\n",
      "{'pr_auc_0': 0.5889916726101043, 'pr_auc_1': 0.5396621572715593, 'pr_auc_2': 0.6793858598361087, 'pr_auc_mean': 0.6026798965725908}\n",
      "8000\n",
      "{'pr_auc_0': 0.5768482524809757, 'pr_auc_1': 0.5423670126987238, 'pr_auc_2': 0.678307244050306, 'pr_auc_mean': 0.5991741697433351}\n",
      "9000\n",
      "{'pr_auc_0': 0.5797983020950073, 'pr_auc_1': 0.5395700427208078, 'pr_auc_2': 0.6816644883504631, 'pr_auc_mean': 0.6003442777220928}\n",
      "10000\n",
      "{'pr_auc_0': 0.578031598668122, 'pr_auc_1': 0.5375227360055858, 'pr_auc_2': 0.6828388156141016, 'pr_auc_mean': 0.5994643834292698}\n",
      "11000\n",
      "{'pr_auc_0': 0.5697928049747482, 'pr_auc_1': 0.537388614827125, 'pr_auc_2': 0.6857196036811435, 'pr_auc_mean': 0.5976336744943388}\n",
      "12000\n",
      "{'pr_auc_0': 0.5651405193662966, 'pr_auc_1': 0.5353584656855014, 'pr_auc_2': 0.68929814949065, 'pr_auc_mean': 0.5965990448474826}\n",
      "13000\n",
      "{'pr_auc_0': 0.5637157173355587, 'pr_auc_1': 0.5368007774426226, 'pr_auc_2': 0.6894872163120757, 'pr_auc_mean': 0.5966679036967523}\n",
      "14000\n",
      "{'pr_auc_0': 0.5673946610453283, 'pr_auc_1': 0.5350481958113189, 'pr_auc_2': 0.6904924019748344, 'pr_auc_mean': 0.5976450862771605}\n",
      "15000\n",
      "{'pr_auc_0': 0.5716868075515934, 'pr_auc_1': 0.5348866993951987, 'pr_auc_2': 0.6889325562907501, 'pr_auc_mean': 0.5985020210791808}\n",
      "16000\n",
      "{'pr_auc_0': 0.5743188743239652, 'pr_auc_1': 0.5352066634557597, 'pr_auc_2': 0.6912526984232588, 'pr_auc_mean': 0.6002594120676612}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "sys.path.append('/liuzicheng/ljh/hyena-dna/')\n",
    "from src.models.sequence.SpliceAI import SpliceAI\n",
    "\n",
    "def genomic_to_one_hot(genomic_sequence):\n",
    "        mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "        one_hot = np.zeros((len(genomic_sequence), 4))\n",
    "        for i, base in enumerate(genomic_sequence):\n",
    "            if base in mapping:\n",
    "                one_hot[i, mapping[base]] = 1\n",
    "            else:\n",
    "                # 如果碱基不是A、C、G、T或N，可以选择将其编码为全零向量或者平均分配概率\n",
    "                one_hot[i, :] = 0.25  # 或者使用 np.full((5,), 0.2) 平均分配概率\n",
    "        return one_hot\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    splice=SpliceAI(3000,1000)\n",
    "    full_sequence=[]\n",
    "    path='/liuzicheng/ljh/hyena-dna/outputs/2024-05-13/05-05-19-201408/checkpoints/val/pr_auc_mean.ckpt'\n",
    "    checkpoint=torch.load(path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    \n",
    "\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    splice.load_state_dict(checkpoint,strict=False)\n",
    "    splice.to('cuda')\n",
    "    splice.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        seq=genomic_to_one_hot(all_seqs[i])\n",
    "        seq_ids = torch.from_numpy(seq).float()[:max_length,:]\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length,-1)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=splice(seqs)\n",
    "        out1_splice=hidden_states.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_splice)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
