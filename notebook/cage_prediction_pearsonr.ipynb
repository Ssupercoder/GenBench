{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "max_length = 2048\n",
    "dataset = load_dataset(\n",
    "            \"InstaDeepAI/genomics-long-range-benchmark\",\n",
    "            task_name='cage_prediction',\n",
    "            sequence_length=max_length,\n",
    "            cache_dir='/liuzicheng/ljh/hyena-dna/data/genomic_long_range',\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "all_seqs=test_dataset['sequence']\n",
    "all_labels=test_dataset['labels']\n",
    "all_Chromosome=test_dataset['chromosome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyena_path='/liuzicheng/ljh/hyena-dna/outputs/2024-04-28/07-05-57-202372/checkpoints/val/pearsonr_cage.ckpt'\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from scipy import stats\n",
    "def pearsonr_cage(outs, y, len_batch=None):\n",
    "    # TODO: generalize, currently for Monash dataset\n",
    "    metrics = []\n",
    "    outs=outs.detach()\n",
    "    # for i in range(50):\n",
    "    #     y_true = y[:, :,i].cpu().numpy()\n",
    "    #     outs_i = outs[:, :,i].cpu().numpy()\n",
    "    \n",
    "    #     r = stats.pearsonr(y_true.flatten(), outs_i.flatten())[0]\n",
    "    #     metrics.append(r)\n",
    "    \n",
    "    for i in range(outs.shape[-1]):\n",
    "        for j in range(outs.shape[0]):\n",
    "            y_true = y[j, :,i].cpu().numpy()\n",
    "            outs_i = outs[j, :,i].cpu().numpy()\n",
    "            \n",
    "        \n",
    "            r=stats.pearsonr(y_true, outs_i)[0]\n",
    "            metrics.append(r)\n",
    "    #output non nan mean of metrics\n",
    "    output=np.nanmean(metrics)\n",
    "\n",
    "    # x_centered = outs - outs.mean(dim = 1, keepdim = True)\n",
    "    # y_centered = y - y.mean(dim = 1, keepdim = True)\n",
    "    # output=F.cosine_similarity(x_centered, y_centered, dim = 1).mean()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "state_dict='/liuzicheng/ljh/hyena-dna/weight/hyenadna/hyenadna-large-1m-seqlen'\n",
    "d_model=256\n",
    "max_length=2048\n",
    "\n",
    "class hyena_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(hyena_model, self).__init__()\n",
    "        self.backbone=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "        self.output_transform = nn.Linear(d_model, 218)\n",
    "        self.linear = nn.Linear(2048,1)\n",
    "        # self.final_pointwise = nn.Sequential(\n",
    "        #     Rearrange('b n d -> b d n'),\n",
    "        #     ConvBlock(d_model, d_model*2, 1),\n",
    "        #     Rearrange('b d n -> b n d'),\n",
    "        #     GELU()\n",
    "        # )\n",
    "        \n",
    "        self.activation=nn.Softplus()\n",
    "    \n",
    "    def forward(self,input_ids,mask=None):\n",
    "        hidden_state=self.backbone(input_ids).last_hidden_state\n",
    "        if mask is None:\n",
    "                restrict = lambda x: (\n",
    "                    torch.cumsum(x, dim=-2)\n",
    "                    / torch.arange(\n",
    "                        1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n",
    "                    ).unsqueeze(-1)\n",
    "                )[..., -1:, :]           \n",
    "        else:\n",
    "                # sum masks\n",
    "                mask_sums = torch.sum(mask, dim=-1).squeeze() - 1  # for 0 indexing\n",
    "\n",
    "                # convert mask_sums to dtype int\n",
    "                mask_sums = mask_sums.type(torch.int64)\n",
    "\n",
    "                restrict = lambda x: (\n",
    "                    torch.cumsum(x, dim=-2)\n",
    "                    / torch.arange(\n",
    "                        1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n",
    "                    ).unsqueeze(-1)\n",
    "                )[torch.arange(x.size(0)), mask_sums, :].unsqueeze(1)  # need to keep original shape\n",
    "        \n",
    "        # hidden_state= (torch.cumsum(hidden_state, dim=-2)\n",
    "        #             / torch.arange(\n",
    "        #                 1, 1 + hidden_state.size(-2), device=hidden_state\n",
    "        #                 .device, dtype=hidden_state.dtype\n",
    "        #             ).unsqueeze(-1)\n",
    "        #         )[..., -1:, :]    \n",
    "        hidden_state=restrict(hidden_state)       \n",
    "        hidden_state = self.output_transform(hidden_state)\n",
    "        output = hidden_state.squeeze(1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# checkpoint=torch.load('/liuzicheng/ljh/hyena-dna/outputs/2024-05-07/08-38-59-295242/checkpoints/val/spearmanr.ckpt')['state_dict']\n",
    "checkpoint=torch.load('/liuzicheng/ljh/hyena-dna/outputs/2024-05-09/04-54-29-787887/checkpoints/val/spearmanr.ckpt')['state_dict']\n",
    "torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.\"\n",
    "        )\n",
    "torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.\"\n",
    "        )\n",
    "hyena=hyena_model().to('cuda')\n",
    "hyena.load_state_dict(checkpoint,strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/liuzicheng/anaconda3/envs/evo-design/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "{'pr_auc_0': 0.5154864462490805, 'pr_auc_1': 0.5590537978461373, 'pr_auc_2': 0.6995160261908133, 'pr_auc_mean': 0.5913520900953437}\n",
      "2000\n",
      "{'pr_auc_0': 0.5141785628747122, 'pr_auc_1': 0.5923108638222884, 'pr_auc_2': 0.7170157376860257, 'pr_auc_mean': 0.6078350547943421}\n",
      "3000\n",
      "{'pr_auc_0': 0.5525250050826079, 'pr_auc_1': 0.5910793756973244, 'pr_auc_2': 0.7207239376237361, 'pr_auc_mean': 0.6214427728012227}\n",
      "4000\n",
      "{'pr_auc_0': 0.5766150348580609, 'pr_auc_1': 0.5790314230703079, 'pr_auc_2': 0.7166946081750826, 'pr_auc_mean': 0.6241136887011504}\n",
      "5000\n",
      "{'pr_auc_0': 0.5867501552443515, 'pr_auc_1': 0.5797426329339977, 'pr_auc_2': 0.7195940933577009, 'pr_auc_mean': 0.6286956271786833}\n",
      "6000\n",
      "{'pr_auc_0': 0.5794178233683753, 'pr_auc_1': 0.5757117317257114, 'pr_auc_2': 0.7160325105593289, 'pr_auc_mean': 0.6237206885511386}\n",
      "7000\n",
      "{'pr_auc_0': 0.582750468792823, 'pr_auc_1': 0.5764519820038508, 'pr_auc_2': 0.7167190441236235, 'pr_auc_mean': 0.6253071649734325}\n",
      "8000\n",
      "{'pr_auc_0': 0.5654796959469311, 'pr_auc_1': 0.5748815847408432, 'pr_auc_2': 0.7169108938634008, 'pr_auc_mean': 0.6190907248503916}\n",
      "9000\n",
      "{'pr_auc_0': 0.571945219126174, 'pr_auc_1': 0.5736231259874266, 'pr_auc_2': 0.7169875949366014, 'pr_auc_mean': 0.620851980016734}\n",
      "10000\n",
      "{'pr_auc_0': 0.572937386171423, 'pr_auc_1': 0.5790547870138508, 'pr_auc_2': 0.7189852320242833, 'pr_auc_mean': 0.6236591350698523}\n",
      "11000\n",
      "{'pr_auc_0': 0.5632058033986864, 'pr_auc_1': 0.5761960827424333, 'pr_auc_2': 0.7207690705968075, 'pr_auc_mean': 0.6200569855793091}\n",
      "12000\n",
      "{'pr_auc_0': 0.5541983569205153, 'pr_auc_1': 0.5747216264801128, 'pr_auc_2': 0.721846128629779, 'pr_auc_mean': 0.6169220373434691}\n",
      "13000\n",
      "{'pr_auc_0': 0.5590395166438484, 'pr_auc_1': 0.5769847517212799, 'pr_auc_2': 0.7226854320431653, 'pr_auc_mean': 0.6195699001360979}\n",
      "14000\n",
      "{'pr_auc_0': 0.5630723350384652, 'pr_auc_1': 0.5751014516891844, 'pr_auc_2': 0.7233495965472467, 'pr_auc_mean': 0.6205077944249654}\n",
      "15000\n",
      "{'pr_auc_0': 0.5728024521214193, 'pr_auc_1': 0.5740064500261025, 'pr_auc_2': 0.7218906088655201, 'pr_auc_mean': 0.6228998370043474}\n",
      "16000\n",
      "{'pr_auc_0': 0.5759293861767032, 'pr_auc_1': 0.5747174826387466, 'pr_auc_2': 0.7231981262168373, 'pr_auc_mean': 0.6246149983440957}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    state_dict='/liuzicheng/ljh/hyena-dna/weight/hyenadna/hyenadna-large-1m-seqlen'\n",
    "    hyena_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    hyena_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(hyena_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    hyena_decoder = nn.Linear(256,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    hyena_model.load_state_dict(checkpoint,strict=False)\n",
    "    hyena_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    hyena_model.eval()\n",
    "    hyena_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "        sequence_encoded=hyena_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=hyena_model(input_ids=seqs).last_hidden_state\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=hyena_decoder(hidden_states)\n",
    "        out1_hyena=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_hyena)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/modules/transformers_modules/dnabert2/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /liuzicheng/ljh/hyena-dna/weight/dnabert2 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/root/.cache/huggingface/modules/transformers_modules/dnabert2/bert_layers.py:433: UserWarning: Increasing alibi size from 512 to 3000\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "{'pr_auc_0': 0.4413639488404158, 'pr_auc_1': 0.6495801693186615, 'pr_auc_2': 0.7035993527238612, 'pr_auc_mean': 0.5981811569609795}\n",
      "2000\n",
      "{'pr_auc_0': 0.4833810037010731, 'pr_auc_1': 0.6579949162945948, 'pr_auc_2': 0.7102444025825281, 'pr_auc_mean': 0.617206774192732}\n",
      "3000\n",
      "{'pr_auc_0': 0.5768825078913379, 'pr_auc_1': 0.6441416139443331, 'pr_auc_2': 0.7067245205190908, 'pr_auc_mean': 0.6425828807849205}\n",
      "4000\n",
      "{'pr_auc_0': 0.6094680668758002, 'pr_auc_1': 0.6325235272635453, 'pr_auc_2': 0.7039271875574811, 'pr_auc_mean': 0.6486395938989422}\n",
      "5000\n",
      "{'pr_auc_0': 0.6276391536145002, 'pr_auc_1': 0.6317029275923831, 'pr_auc_2': 0.7048822524375558, 'pr_auc_mean': 0.6547414445481463}\n",
      "6000\n",
      "{'pr_auc_0': 0.6320016902614787, 'pr_auc_1': 0.6241559409749575, 'pr_auc_2': 0.6982471967887951, 'pr_auc_mean': 0.6514682760084104}\n",
      "7000\n",
      "{'pr_auc_0': 0.6455787283720203, 'pr_auc_1': 0.6287807207037904, 'pr_auc_2': 0.7012021169564384, 'pr_auc_mean': 0.6585205220107498}\n",
      "8000\n",
      "{'pr_auc_0': 0.6472556370414385, 'pr_auc_1': 0.6290027698675833, 'pr_auc_2': 0.703146369672784, 'pr_auc_mean': 0.6598015921939352}\n",
      "9000\n",
      "{'pr_auc_0': 0.6453219208812204, 'pr_auc_1': 0.6288617347625843, 'pr_auc_2': 0.7007478522560913, 'pr_auc_mean': 0.6583105026332986}\n",
      "10000\n",
      "{'pr_auc_0': 0.6470114523640572, 'pr_auc_1': 0.6358241896759768, 'pr_auc_2': 0.7035487965867486, 'pr_auc_mean': 0.6621281462089276}\n",
      "11000\n",
      "{'pr_auc_0': 0.6390487075744324, 'pr_auc_1': 0.6360777592555877, 'pr_auc_2': 0.7045179479767065, 'pr_auc_mean': 0.6598814716022422}\n",
      "12000\n",
      "{'pr_auc_0': 0.626828593713372, 'pr_auc_1': 0.6380907263073617, 'pr_auc_2': 0.7059928624351631, 'pr_auc_mean': 0.6569707274852989}\n",
      "13000\n",
      "{'pr_auc_0': 0.6336316624652701, 'pr_auc_1': 0.6381459072859856, 'pr_auc_2': 0.7070680443183892, 'pr_auc_mean': 0.6596152046898817}\n",
      "14000\n",
      "{'pr_auc_0': 0.6384097449602768, 'pr_auc_1': 0.6372235322340396, 'pr_auc_2': 0.7073622405396568, 'pr_auc_mean': 0.6609985059113245}\n",
      "15000\n",
      "{'pr_auc_0': 0.6440681559117699, 'pr_auc_1': 0.6339020425904798, 'pr_auc_2': 0.706046603988664, 'pr_auc_mean': 0.6613389341636379}\n",
      "16000\n",
      "{'pr_auc_0': 0.6460772733413054, 'pr_auc_1': 0.6352172297748268, 'pr_auc_2': 0.7079444804071149, 'pr_auc_mean': 0.6630796611744156}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='/liuzicheng/ljh/hyena-dna/weight/dnabert2'\n",
    "    bert2_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    bert2_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(bert2_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    bert2_decoder = nn.Linear(768,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    bert2_model.load_state_dict(checkpoint,strict=False)\n",
    "    bert2_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    bert2_model.eval()\n",
    "    bert2_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=bert2_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=bert2_model(input_ids=seqs,export_hidden_states=True)[0]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=bert2_decoder(hidden_states)\n",
    "        out1_bert2=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_bert2)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "{'pr_auc_0': 0.7880212471259458, 'pr_auc_1': 0.6770300582523869, 'pr_auc_2': 0.7187810802183864, 'pr_auc_mean': 0.7279441285322396}\n",
      "2000\n",
      "{'pr_auc_0': 0.8167638615468158, 'pr_auc_1': 0.6561324144945506, 'pr_auc_2': 0.7148380653496311, 'pr_auc_mean': 0.7292447804636658}\n",
      "3000\n",
      "{'pr_auc_0': 0.8478734619509036, 'pr_auc_1': 0.6513071565600782, 'pr_auc_2': 0.7241171135823166, 'pr_auc_mean': 0.7410992440310995}\n",
      "4000\n",
      "{'pr_auc_0': 0.851880441791636, 'pr_auc_1': 0.6420799196727746, 'pr_auc_2': 0.7283626722476464, 'pr_auc_mean': 0.7407743445706857}\n",
      "5000\n",
      "{'pr_auc_0': 0.8597831673439507, 'pr_auc_1': 0.6354504501119524, 'pr_auc_2': 0.7304985274249985, 'pr_auc_mean': 0.7419107149603006}\n",
      "6000\n",
      "{'pr_auc_0': 0.8677362138412498, 'pr_auc_1': 0.6283971543377364, 'pr_auc_2': 0.7241814758055377, 'pr_auc_mean': 0.7401049479948413}\n",
      "7000\n",
      "{'pr_auc_0': 0.8611445707305545, 'pr_auc_1': 0.6311706598652306, 'pr_auc_2': 0.7273982069457894, 'pr_auc_mean': 0.7399044791805247}\n",
      "8000\n",
      "{'pr_auc_0': 0.8626132615327134, 'pr_auc_1': 0.6307871423897916, 'pr_auc_2': 0.7270191051685777, 'pr_auc_mean': 0.7401398363636943}\n",
      "9000\n",
      "{'pr_auc_0': 0.8637529334097808, 'pr_auc_1': 0.6275317471304522, 'pr_auc_2': 0.7256472634547031, 'pr_auc_mean': 0.7389773146649787}\n",
      "10000\n",
      "{'pr_auc_0': 0.8660732716980265, 'pr_auc_1': 0.632267222086135, 'pr_auc_2': 0.726126224183549, 'pr_auc_mean': 0.7414889059892368}\n",
      "11000\n",
      "{'pr_auc_0': 0.8634528897797865, 'pr_auc_1': 0.6318687763955713, 'pr_auc_2': 0.7274922765831349, 'pr_auc_mean': 0.7409379809194975}\n",
      "12000\n",
      "{'pr_auc_0': 0.8624209168105471, 'pr_auc_1': 0.6325031604480222, 'pr_auc_2': 0.7280860023792299, 'pr_auc_mean': 0.7410033598792665}\n",
      "13000\n",
      "{'pr_auc_0': 0.8617312584465523, 'pr_auc_1': 0.6318345905976037, 'pr_auc_2': 0.7295563679336788, 'pr_auc_mean': 0.7410407389926116}\n",
      "14000\n",
      "{'pr_auc_0': 0.8629787419300166, 'pr_auc_1': 0.629811678170904, 'pr_auc_2': 0.7319049070207474, 'pr_auc_mean': 0.7415651090405561}\n",
      "15000\n",
      "{'pr_auc_0': 0.8648920318875633, 'pr_auc_1': 0.6275538667357794, 'pr_auc_2': 0.7294159770745242, 'pr_auc_mean': 0.7406206252326223}\n",
      "16000\n",
      "{'pr_auc_0': 0.8667097413780834, 'pr_auc_1': 0.6298151777649452, 'pr_auc_2': 0.7303904047683528, 'pr_auc_mean': 0.7423051079704606}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='/liuzicheng/ljh/hyena-dna/weight/genalm/gena-lm-bigbird-base-t2t'\n",
    "    genalm_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    genalm_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(genalm_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    genalm_decoder = nn.Linear(768,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    genalm_model.load_state_dict(checkpoint,strict=False)\n",
    "    genalm_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    genalm_model.eval()\n",
    "    genalm_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=genalm_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=genalm_model(input_ids=seqs, output_hidden_states=True,).hidden_states[-1]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=genalm_decoder(hidden_states)\n",
    "        out1_genalm=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_genalm)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='/liuzicheng/ljh/hyena-dna/weight/nt/nucleotide-transformer-v2-500m-multi-species'\n",
    "    nt_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    nt_model=AutoModelForMaskedLM.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(NT_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    nt_decoder = nn.Linear(1024,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    nt_model.load_state_dict(checkpoint,strict=False)\n",
    "    nt_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    nt_model.eval()\n",
    "    nt_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=nt_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=nt_model(input_ids=seqs,output_hidden_states=True)['hidden_states'][-1]\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=nt_decoder(hidden_states)\n",
    "        out1_nt=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_nt)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "max_length=3000\n",
    "with torch.no_grad():\n",
    "    state_dict='/liuzicheng/ljh/hyena-dna/weight/mamba/caduceus-ph_seqlen-131k_d_model-256_n_layer-16'\n",
    "    mamba_tokenizer=AutoTokenizer.from_pretrained(state_dict, trust_remote_code=True)\n",
    "    mamba_model=AutoModel.from_pretrained(state_dict, trust_remote_code=True).to('cuda')\n",
    "    full_sequence=[]\n",
    "    checkpoint=torch.load(mamba_path)['state_dict']\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"model.backbone.\"\n",
    "        )\n",
    "    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            checkpoint, \"decoder.0.output_transform.\"\n",
    "        )\n",
    "\n",
    "    mamba_decoder = nn.Linear(256,3).to('cuda')\n",
    "    #edit key name in hyena_decoder\n",
    "    \n",
    "    mamba_model.load_state_dict(checkpoint,strict=False)\n",
    "    mamba_decoder.load_state_dict(checkpoint,strict=False)\n",
    "    mamba_model.eval()\n",
    "    mamba_decoder.eval()\n",
    "\n",
    "    target_list=[]\n",
    "    seq_list=[]\n",
    "    for i in range(len(all_seqs)):\n",
    "\n",
    "        sequence_encoded=mamba_tokenizer(all_seqs[i],\n",
    "                            add_special_tokens= False,  # this is what controls adding eos\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        seq_ids=sequence_encoded['input_ids']\n",
    "        seq_ids = torch.LongTensor(seq_ids)\n",
    "        target = all_labels[i][:1000]\n",
    "        \n",
    "        seqs=torch.reshape(seq_ids,(1,max_length)).to('cuda')\n",
    "        target_list.append(target)\n",
    "        hidden_states=mamba_model(seqs,output_hidden_states=True).last_hidden_state\n",
    "        hidden_states=hidden_states[..., :1000, :]\n",
    "        out1=mamba_decoder(hidden_states)\n",
    "        out1_mamba=out1.squeeze(1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        seq_list.append(out1_mamba)\n",
    "        seq_list_numpy=np.array(seq_list)\n",
    "\n",
    "        target_list_numpy=np.array(target_list)\n",
    "        seq_list_tensor=torch.FloatTensor(seq_list_numpy)\n",
    "        target_list_tensor=torch.FloatTensor(target_list_numpy)\n",
    "        #calculate the \n",
    "        if i>=1:\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(i)\n",
    "                pearsonr=pr_auc(seq_list_tensor,target_list_tensor)\n",
    "                print(pearsonr)\n",
    "            \n",
    "\n",
    "#plot the bar plot of the pearsonr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
